{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZkmsny2PCp6",
        "outputId": "2e5656a4-1f18-42ed-d2a0-0b76bb5296ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Fc7h0ImyczPd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from faker import Faker\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCw3pLJ-dlNa"
      },
      "outputs": [],
      "source": [
        "def setup_nltk():\n",
        "    \"\"\"Setup NLTK by downloading required data silently.\"\"\"\n",
        "    try:\n",
        "        # Download required NLTK data \n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading NLTK data: {e}\")\n",
        "        print(\"Please run this command manually:\")\n",
        "        print(\"import nltk; nltk.download('punkt'); nltk.download('punkt_tab')\")\n",
        "        sys.exit(1)\n",
        "\n",
        "MAX_TOKENS = 4000  # Maximum tokens allowed in context window\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3ounivhadnnh"
      },
      "outputs": [],
      "source": [
        "def count_tokens(text):\n",
        "    \"\"\"Count the number of tokens in text.\"\"\"\n",
        "    try:\n",
        "        return len(text.split())  # Using simple split as fallback\n",
        "    except:\n",
        "        return len(text)\n",
        "\n",
        "def split_tokens(text, max_length):\n",
        "    \"\"\"Split text into chunks of specified maximum length.\"\"\"\n",
        "    words = text.split()  # Using simple split instead of word_tokenize\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), max_length):\n",
        "        chunk = ' '.join(words[i:i + max_length])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpOV4bOgdpOR"
      },
      "outputs": [],
      "source": [
        "def create_summary(text, target_length=None):\n",
        "    \"\"\"\n",
        "    Create a summary of the text. If target_length is specified,\n",
        "    truncate to that length. Otherwise, return original text.\n",
        "    \"\"\"\n",
        "    if not target_length:\n",
        "        return text\n",
        "\n",
        "    words = text.split()  \n",
        "    return ' '.join(words[:target_length])\n",
        "\n",
        "def calculate_target_lengths(len1, len2, total_target):\n",
        "    \"\"\"Calculate proportional target lengths for two texts.\"\"\"\n",
        "    total = len1 + len2\n",
        "    if total == 0:\n",
        "        return 0, 0\n",
        "\n",
        "    prop1 = len1 / total\n",
        "    prop2 = len2 / total\n",
        "\n",
        "    return (\n",
        "        int(total_target * prop1),\n",
        "        int(total_target * prop2)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "E7SFz1B5drTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def summarize_long_text(text, target_length):\n",
        "    \"\"\"\n",
        "    Summarize long text to fit within target length using\n",
        "    a simple hierarchical approach.\n",
        "    \"\"\"\n",
        "    # If text is already short enough, return it\n",
        "    if count_tokens(text) <= target_length:\n",
        "        return text\n",
        "\n",
        "    # Split into chunks and summarize each\n",
        "    chunks = split_tokens(text, target_length)\n",
        "    summaries = [create_summary(chunk, target_length // len(chunks))\n",
        "                for chunk in chunks]\n",
        "\n",
        "    # Combine summaries\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # If still too long, summarize again\n",
        "    if count_tokens(final_summary) > target_length:\n",
        "        final_summary = create_summary(final_summary, target_length)\n",
        "\n",
        "    return final_summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EhJiqCeBdubn"
      },
      "outputs": [],
      "source": [
        "def save_text(text, filename):\n",
        "    \"\"\"Save text to file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as file:\n",
        "            file.write(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file {filename}: {e}\")\n",
        "\n",
        "def show_summary(filename, num_sentences=5):\n",
        "    \"\"\"Show first few sentences of a file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            # Simple sentence splitting as fallback\n",
        "            sentences = text.split('. ')[:num_sentences]\n",
        "            print(f\"\\n=== {filename} ===\")\n",
        "            print('. '.join(sentences) + '.')\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filename}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvsjdOQdwEs",
        "outputId": "3b5b1183-97b2-49e9-e6f4-93bca81778a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summaries created successfully!\n",
            "\n",
            "=== summary1.txt ===\n",
            "Produce rather difficult. Several important over agency democratic smile. Political recent it glass wish method wind. Source and herself former work of science. Safe activity television those suggest effect area.\n",
            "\n",
            "=== summary2.txt ===\n",
            "But listen myself less usually within. Anything international magazine modern month small. Short event dog collection. Fast up light increase. Effect himself price free would traditional question.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup NLTK\n",
        "setup_nltk()\n",
        "\n",
        "# Generate sample texts\n",
        "fake = Faker()\n",
        "text1 = \" \".join(fake.paragraphs(nb=200))\n",
        "text2 = \" \".join(fake.paragraphs(nb=500))\n",
        "\n",
        "# Get lengths and calculate targets\n",
        "len1 = count_tokens(text1)\n",
        "len2 = count_tokens(text2)\n",
        "target1, target2 = calculate_target_lengths(len1, len2, MAX_TOKENS)\n",
        "\n",
        "# Create and save summaries\n",
        "summary1 = summarize_long_text(text1, target1)\n",
        "summary2 = summarize_long_text(text2, target2)\n",
        "\n",
        "save_text(summary1, 'summary1.txt')\n",
        "save_text(summary2, 'summary2.txt')\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSummaries created successfully!\")\n",
        "show_summary('summary1.txt')\n",
        "show_summary('summary2.txt')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
